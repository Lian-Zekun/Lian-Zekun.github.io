---
title: 'python爬虫---爬虫前备知识'
date: 2019-02-12
categories: [计算机]
tags: [Python, 爬虫]
media_subpath: /assets/img/posts/2019-02-12-python爬虫---爬虫前备知识/
image: ../../PreviewImage/爬虫.jpg
---

## URL

URL的全称为Universal Resource Locator ，即统一资源定位符。当我们访问任一网站时，浏览器上方都会有相应的链接，例如当我们进入百度首页是，上方的链接为:https://www.baidu.com/ ,而 https://www.baidu.com/ 就是一个 URL，方便我们能够从网上查找资源。

## 网页

当我们通过 URL 进入指定的网站后，我们在浏览器里就看到了网页。
网页可以分为三大部分HTML、CSS 和 JavaScript 。
其中 HTML 定义了网页的内容和结构，即通过一系列标签来描述显示的东西，例如img 显示图片，p 指定显示段落等 。
CSS 描述了网页的布局，因为只有 HTML 的页面布局并不美观，通过CSS对网页页面进行排版。
JavaScript 定义了网页的行为，即让我们在网页里可能会看到一些交互和动画效果。
我们最终在浏览器中看到的就是经由CSS和JavaScript加工完成的HTML页面。
我们在浏览器中如何查看网页的源码？以Chrome浏览器为例，我们进入百度的首页，右击任一地方井选择“检查”项(或者直接按快捷键Fl2)，即可打开浏览器的开发者工具，这时在 Elements 选项卡即可看到当前网页的源代码。![爬虫工作原理示意图](20190310093023525.jpg)

## HTTP请求过程

我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容。 实际上，这个过程是浏览器向网站所在的服务器发送了一个请求，网站服务器接收到这个请求后进行处理和解析，然后返回对应的响应，接着传回给浏览器 。 响应里包含了页面的源代码等内容，浏览器再对其进行解析，便将网页呈现了出来。

### 请求

请求，由客户端向服务端发出，可以分为 4 部分内容:请求方法( Request Method ) 、 请求的网址
( Request URL )、请求头( Request Headers ) 、请求体( Request Body )。

#### 1. 请求方法

 常见的请求方法有两种 : GET和POST 。
 GET和POST请求方法有如下区别 。
     - GET请求中的参数包含在 URL 里面，数据可以在 URL 中看到，而 POST 请求的 URL 不会包
含这些数据，数据都是通过表单形式传输的，会包含在请求体中 。
    - GET请求提交的数据最多只有 1024 字节，而 POST 方式没有限制 。

一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，使用 GET 方式请求的话，密码就会暴露在 URL 里面，造成密码泄露，所以这里最好以 POST 方式发送 。上传文件时，由于文件内容比较大，也会选用 POST 方式 。
我们平常遇到的绝大部分请求都是 GET 或 POST 请求，另外还有一些请求方法，如 GET 、HEAD 、POST 、PUT 、DELETE 、OPTIONS 、CONNECT 、TRACE 等。

#### 2. 请求的网址

请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源 。

#### 3. 请求头

请求头，用来说明服务器要使用的附加信息，比较重要的信息有 Cookie 、Referer 、User-Agent 等 。
下面简要说明一些常用的头信息 。
    - **Accept** :请求报头域,用于指定客户端可接受哪些类型的信息 。
    - **Accept-Language** :指定客户端可接受的语言类型 。
    - **Accept-Encoding** :指定客户端可接受的内容编码 。
    - **Host** :用于指定请求资源的主机 IP 和端口号，其内容为请求 URL 的原始服务器或网关的位置。从 HTTP 1. l 版本开始,请求必须包含此内容。
    - **Cookie** :也常用复数形式 Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据 。它的主要功能是维持当前访问会话 。
    - **User-Agent** :简称 UA ，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本 、浏览器及版本等信息 。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别为爬虫 。

#### 4. 请求体

请求体一般承载的内容是 POST 请求中的表单数据，而对于 GET 请求,请求体则为空 。

### 响应

响应，由服务端返回给客户端，可以分为三部分:响应状态码( Response Status Code )、响应头
( Response Headers )和响应体( Response Body )。

#### 1. 响应状态码

响应状态码表示服务器的响应状态，如 200 代表服务器正常响应，404 代表页面未找到，500 代表
服务器内部发生错误 。在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为 200，则
证明成功返回数据，再进行进一步的处理，否则直接忽略 。

#### 2. 响应头

响应头包含了服务器对请求的应答信息，下面简要说明一些常用的头信息 。
    - **Content-Encoding** : 指定响应内容的编码 。
    - **Content-Type** : 文档类型，指定返 回的数据类型是什么，如 text/html 代表返回 HTML 文档，
application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回图片 。
    - **Set-Cookie** : 设置 Cookies，响应头 中的 Set-Cookie 告诉浏览器需要将此内容放在 Cookies
中，下次请求携带 Cookies 请求 。

#### 3. 响应体

最重要的当属响应体的内容了。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的 HTML 代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体。

## 爬虫概述

简单来说，爬虫就是获取网页并提取和保存信息的自动化程序。

### 1. 获取网页

爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了 。
前面讲了请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个请求并发送给服务器，然后接收到响应并将其解析出来，那么这个流程怎样实现呢?总不能手工去截取网页源码吧?
不用担心，Python 提供了许多库来帮助我们实现这个操作，如 urllib 、requests 等。我们可以用这些库来帮助我们实现 HTTP 请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中 的 Body 部分即可，即得到网页的源代码,这样我们可以用程序来实现获取网页的过程了。

### 2. 提取信息

获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。首先，最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错(博主低能基本不会)。另外，由于网页的结构有一定的规则 ，所以还有一些根据网页节点属性(所有标签定义的内容都是节点)、CSS 选择器或 XPath 来提取网页信息的库，如 Beautiful Soup 、pyquery 、lxml 等 。使用这些库，我们可以高效快速地从中提取网页信息,如节点的属性、文本值等 。
提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰 ，以便我们后续处理和分析数据 。

### 3. 保存数据

提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等。

### 4. 自动化程序

说到自动化程序，意思是说爬虫可以代替人来完成这些操作。首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。

## 库的使用

本来博主想再写一篇文章来介绍requests库和XPath的使用，但写到一半发现也只能是照搬文档，毫无新意，在此还是献上文档链接，大家自行学习。
[Requests: 让 HTTP 服务人类](https://docs.python-requests.org/en/latest/index.html)
[XPath 教程 | 菜鸟教程](https://www.runoob.com/xpath/xpath-tutorial.html)

## 参考书籍

本文大部分思路及定义均来自<<pytho3网络爬虫开发实战>>

***

本博客文章仅供博主学习交流，博主才疏学浅，语言表达能力有限，对知识的理解、编写代码能力都有很多不足，希望各路大神多多包涵，多加指点。

